{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOMHJ7bwkAqQ80AaxxEMd1h",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Satyendra0207/Text-and-image_analytics/blob/main/1_text_analysis_basic_a.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **TEXT PREPROCESSING**"
      ],
      "metadata": {
        "id": "aKzLEP1k72NR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1.   first step in the pipeline of NLP(natural language processing)\n",
        "2.   **Text Preprocessing is the process of bringing the text into a form that is predictable and analyzable for a specific task. A task is the combination of approach and domain**\n",
        "3.   For example, extracting top keywords with TF-IDF (approach) from Tweets (domain) is an example of a task. The main objective of text preprocessing is to break the text into a form that machine learning algorithms can digest.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "G5v2pXsA8AYH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Noise removal**\n",
        "Noise removal is about **removing digits**, **characters**, and **pieces of text** that interfere with the process of text analysis."
      ],
      "metadata": {
        "id": "IoWv2ypW-W0_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eNyzHF2IJDg2"
      },
      "outputs": [],
      "source": [
        "# import the necessary libraries\n",
        "import nltk\n",
        "import string\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def text_lowercase(text):\n",
        "    return text.lower()\n",
        "\n",
        "def text_upper(text):\n",
        "  return text.upper() #for upper case \n",
        " \n",
        "input_str = \"Hey, did you know that the summer break is coming? Amazing right !! It's only 5 more days !!\"\n",
        "print(text_lowercase(input_str))\n",
        "print(text_upper(input_str))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQ47l3h4JD6s",
        "outputId": "75acbe61-5405-4624-b4b0-9f73302a58cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hey, did you know that the summer break is coming? amazing right !! it's only 5 more days !!\n",
            "HEY, DID YOU KNOW THAT THE SUMMER BREAK IS COMING? AMAZING RIGHT !! IT'S ONLY 5 MORE DAYS !!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove numbers\n",
        "def remove_numbers(text):\n",
        "    result = re.sub(r'\\d+', '', text) #\\d+ ---any digit of any sequence \n",
        "    return result\n",
        " \n",
        "input_str = \"There are 3 balls in this bag, and 12 in the other one.\"\n",
        "remove_numbers(input_str)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "0fSzoWOVJD9i",
        "outputId": "c0106500-487a-4d5e-f81c-0461e490820c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'There are  balls in this bag, and  in the other one.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Numbers to digit conversion"
      ],
      "metadata": {
        "id": "e7w049MeEdM_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import the inflect library\n",
        "import inflect\n",
        "p = inflect.engine()\n",
        " \n",
        "# convert number into words\n",
        "def convert_number(text):\n",
        "    # split string into list of words\n",
        "    temp_str = text.split()\n",
        "    # initialise empty list\n",
        "    new_string = []\n",
        " \n",
        "    for word in temp_str:\n",
        "        # if word is a digit, convert the digit\n",
        "        # to numbers and append into the new_string list\n",
        "        if word.isdigit():\n",
        "            temp = p.number_to_words(word)\n",
        "            new_string.append(temp)\n",
        "        # append the word as it is\n",
        "        else:\n",
        "            new_string.append(word)\n",
        " \n",
        "    # join the words of new_string to form a string\n",
        "    temp_str = ' '.join(new_string)\n",
        "    return temp_str\n",
        " \n",
        "input_str = 'There are 3 balls in this bag, and 1216 in the other one.'\n",
        "convert_number(input_str)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "fGGjS5G9JEDO",
        "outputId": "628186ca-3ef0-4b29-d493-49fa99562605"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'There are three balls in this bag, and one thousand, two hundred and sixteen in the other one.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# remove punctuation\n",
        "#punctuatuion----!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
        "def remove_punctuation(text):\n",
        "    translator = str.maketrans('','',string.punctuation)\n",
        "    return text.translate(translator)\n",
        " \n",
        "input_str = \"Hey, did you know that the summer break is coming? Amazing right !! It's only 5 more days !!\"\n",
        "remove_punctuation(input_str)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "l6VweQ-VJEGL",
        "outputId": "0131b52d-fd3c-413f-e1bf-5d91d07dd591"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hey did you know that the summer break is coming Amazing right  Its only 5 more days '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# remove whitespace from text\n",
        "def remove_whitespace(text):\n",
        "    #return(re.sub(r\"\\s+\",\" \", text, flags = re.I))\n",
        "    return  \" \".join(text.split())\n",
        " \n",
        "input_str = \"   we don't need   the given questions\"\n",
        "remove_whitespace(input_str)"
      ],
      "metadata": {
        "id": "pycsVzkpJEIy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2247222b-d8c4-4317-fac7-13c9a0599c2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" we don't need the given questions\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bNh3bEE_Wx7H",
        "outputId": "354d1703-da33-4035-85cc-f4b1b558b6ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenization** is defined as a process to split the text into smaller units, i.e., tokens, perhaps at the same time throwing away certain characters, such as punctuation. Tokens could be words, numbers, symbols, n-grams, or characters. N-grams are a combination of n words or characters together. Tokenization does this task by locating word boundaries."
      ],
      "metadata": {
        "id": "Y_gmHGWo-Hk-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Languages such as English and French are referred to as space-delimited as most of the words are separated from each other by space. Languages such as Chinese and Thai are said to be unsegmented as words do not have clear boundaries Tokenization is also affected by writing systems. Structures of languages can be grouped into three categories:\n",
        "\n",
        "**Isolating:** Words do not divide into smaller units. Example: Mandarin\n",
        "\n",
        "**Agglutinative**: Words divide into smaller units. Example: Japanese, Tamil\n",
        "\n",
        "**Inflectional:** Boundaries between morphemes are not clear and ambiguous in terms of grammatical meaning. Example: Latin"
      ],
      "metadata": {
        "id": "qF4RKf_y_S_S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "sentence= \"Hey nice to see you! Have a great day\"\n",
        "token=word_tokenize(sentence)\n",
        "count = Counter(token)\n",
        "print(\"len(count) = \",len(count)) # %s placeholder for string or used for concatenation\n",
        "print(\"most_common = \",count.most_common(10))# here 10, means top 10 out of total\n",
        "print(token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v4LOTErI_vd3",
        "outputId": "dab3d2fe-7130-4af8-ae04-2c88931ce863"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len(count) = %s 10\n",
            "most_common = %s [('Hey', 1), ('nice', 1), ('to', 1), ('see', 1), ('you', 1), ('!', 1), ('Have', 1), ('a', 1), ('great', 1), ('day', 1)]\n",
            "['Hey', 'nice', 'to', 'see', 'you', '!', 'Have', 'a', 'great', 'day']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Remove default stopwords:**\n",
        "\n",
        "Stopwords are words that do not contribute to the meaning of a sentence. Hence, they can safely be removed without causing any change in the meaning of the sentence. The NLTK library has a set of stopwords and we can use these to remove stopwords from our text and return a list of word tokens."
      ],
      "metadata": {
        "id": "aZlcgvCwWSZ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        " \n",
        "# remove stopwords function\n",
        "def remove_stopwords(text):\n",
        "    stop_words = set(stopwords.words(\"english\")) # contains all the word of english that doesn't have meaning the sentence\n",
        "    word_tokens = word_tokenize(text) #gives a token every word/letter in the text--delimiter -- space\n",
        "    print(word_tokens)\n",
        "    filtered_text = [word for word in word_tokens if word not in stop_words]\n",
        "    return filtered_text\n",
        " \n",
        "example_text = \"This is a sample sentence and we are going to remove the stopwords from this.\"\n",
        "remove_stopwords(example_text)"
      ],
      "metadata": {
        "id": "FaLZ-5uCJELq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0567951a-3e1a-44c7-958b-40781fac2436"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This', 'is', 'a', 'sample', 'sentence', 'and', 'we', 'are', 'going', 'to', 'remove', 'the', 'stopwords', 'from', 'this', '.']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['This', 'sample', 'sentence', 'going', 'remove', 'stopwords', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Normalization**\n",
        "Normalization is the process of converting the token into its basic form (morpheme). Inflection is removed from the token to get the base form of the word. It helps in reducing the number of unique tokens and redundancy in the data. It reduces the data dimensionality and removes the variation of a word from the text.\n",
        "\n",
        "There are two techniques to perform normalization. They are **Stemming** and  **Lemmatization**."
      ],
      "metadata": {
        "id": "qW6K-DGdBMWI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stemming:**\n",
        "\n",
        "Stemming is the process of getting the root form of a word. Stem or root is the part to which inflectional affixes (-ed, -ize, -de, -s, etc.) are added. The stem of a word is created by removing the prefix or suffix of a word. So, stemming a word may not result in actual words.\n",
        "\n",
        "books      --->    book ,\n",
        "looked     --->    look ,\n",
        "denied     --->    deni ,\n",
        "flies      --->    fli"
      ],
      "metadata": {
        "id": "VU8gduhGYA8N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "stemmer = PorterStemmer()\n",
        " \n",
        "# stem words in the list of tokenized words\n",
        "def stem_words(text):\n",
        "    word_tokens = word_tokenize(text)\n",
        "    stems = [stemmer.stem(word) for word in word_tokens]\n",
        "    return stems\n",
        " \n",
        "text = 'data science uses scientific methods algorithms and many types of processes'\n",
        "stem_words(text)"
      ],
      "metadata": {
        "id": "ovNKxgELJEOe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e653f34a-2617-45c6-d84c-a9f161053fd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['data',\n",
              " 'scienc',\n",
              " 'use',\n",
              " 'scientif',\n",
              " 'method',\n",
              " 'algorithm',\n",
              " 'and',\n",
              " 'mani',\n",
              " 'type',\n",
              " 'of',\n",
              " 'process']"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lemmatization:**\n",
        "\n",
        "Like stemming, lemmatization also converts a word to its root form. **The only difference is that lemmatization ensures that the root word belongs to the language.** We will get valid words if we use lemmatization. In NLTK, we use the WordNetLemmatizer to get the lemmas of words. We also need to provide a context for the lemmatization. So, we add the part-of-speech as a parameter."
      ],
      "metadata": {
        "id": "ivnb5tEQYvRm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nSZYtZtpZGn0",
        "outputId": "b4e28dca-1a1d-4d78-fa57-100c9edb1096"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "# lemmatize string\n",
        "def lemmatize_word(text):\n",
        "    word_tokens = word_tokenize(text)\n",
        "    # provide context i.e. part-of-speech\n",
        "    lemmas = [lemmatizer.lemmatize(word, pos ='v') for word in word_tokens]\n",
        "    return lemmas\n",
        " \n",
        "text = 'data science uses scientific methods algorithms and many types of processes'\n",
        "lemmatize_word(text)"
      ],
      "metadata": {
        "id": "UvhxsOGBJERW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80337b11-6717-4590-9039-cf0d5b0a8d9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['data',\n",
              " 'science',\n",
              " 'use',\n",
              " 'scientific',\n",
              " 'methods',\n",
              " 'algorithms',\n",
              " 'and',\n",
              " 'many',\n",
              " 'type',\n",
              " 'of',\n",
              " 'process']"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part of Speech Tagging:**\n",
        "\n",
        "The part of speech explains **how a word is used in a sentence**. In a sentence, a word can have different contexts and semantic meanings. The **basic natural language processing models like bag-of-words ** fail to identify these relations between words. Hence, we use part of speech tagging to mark a word to its part of speech tag based on its context in the data. It is also used to extract relationships between words."
      ],
      "metadata": {
        "id": "2kryPQerZqUO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l7_TKxafaZy1",
        "outputId": "77f7b2d0-f4d0-4b20-aeb7-d624ddc126c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "  \n",
        "# convert text into word_tokens with their tags\n",
        "def pos_tagging(text):\n",
        "    word_tokens = word_tokenize(text)\n",
        "    return pos_tag(word_tokens)\n",
        "  \n",
        "pos_tagging('You just gave me a scare')\n",
        "pos_tagging('listen to me I will not call you')"
      ],
      "metadata": {
        "id": "hDleJhpeJEUc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49c89a66-dd28-4287-aa33-62b9a853c99b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('listen', 'NN'),\n",
              " ('to', 'TO'),\n",
              " ('me', 'PRP'),\n",
              " ('I', 'PRP'),\n",
              " ('will', 'MD'),\n",
              " ('not', 'RB'),\n",
              " ('call', 'VB'),\n",
              " ('you', 'PRP')]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can get the details of all the part of speech tags using the Penn Treebank tagset."
      ],
      "metadata": {
        "id": "lOPcGaLkbQdU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download the tagset \n",
        "nltk.download('tagsets')\n",
        "  \n",
        "# extract information about the tag\n",
        "nltk.help.upenn_tagset('NN')"
      ],
      "metadata": {
        "id": "pI8IZioPJEXd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e58b9677-43f3-403e-83ef-a022735e9f79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package tagsets to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NN: noun, common, singular or mass\n",
            "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
            "    investment slide humour falloff slick wind hyena override subhumanity\n",
            "    machinist ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data]   Unzipping help/tagsets.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Chunking:**\n",
        "\n",
        "Chunking is the process of **extracting phrases from unstructured text** and more structure to it. It is also **known as shallow parsing**. It is done on top of Part of Speech tagging. It groups word into “chunks”, mainly of noun phrases. Chunking is done using regular expressions."
      ],
      "metadata": {
        "id": "Gi76EfVnbeIP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tkinter as tk"
      ],
      "metadata": {
        "id": "EuKuk_EYLKqS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize \n",
        "from nltk import pos_tag\n",
        "  \n",
        "# define chunking function with text and regular\n",
        "# expression representing grammar as parameter\n",
        "def chunking(text, grammar):\n",
        "    word_tokens = word_tokenize(text)\n",
        "  \n",
        "    # label words with part of speech\n",
        "    word_pos = pos_tag(word_tokens)\n",
        "    print(word_pos)\n",
        "    # create a chunk parser using grammar\n",
        "    chunkParser = nltk.RegexpParser(grammar)\n",
        "    print(chunkParser)\n",
        "    # test it on the list of word tokens with tagged pos\n",
        "    tree = chunkParser.parse(word_pos) \n",
        "    for subtree in tree.subtrees():\n",
        "        print(subtree)\n",
        "    tree.draw()\n",
        "      \n",
        "sentence = 'the little yellow bird is flying in the sky'\n",
        "grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
        "chunking(sentence, grammar)"
      ],
      "metadata": {
        "id": "vgaTwXM6JEaV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 554
        },
        "outputId": "d9b813ac-2c79-425f-86be-1315dbd890f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('the', 'DT'), ('little', 'JJ'), ('yellow', 'JJ'), ('bird', 'NN'), ('is', 'VBZ'), ('flying', 'VBG'), ('in', 'IN'), ('the', 'DT'), ('sky', 'NN')]\n",
            "chunk.RegexpParser with 1 stages:\n",
            "RegexpChunkParser with 1 rules:\n",
            "       <ChunkRule: '<DT>?<JJ>*<NN>'>\n",
            "(S\n",
            "  (NP the/DT little/JJ yellow/JJ bird/NN)\n",
            "  is/VBZ\n",
            "  flying/VBG\n",
            "  in/IN\n",
            "  (NP the/DT sky/NN))\n",
            "(NP the/DT little/JJ yellow/JJ bird/NN)\n",
            "(NP the/DT sky/NN)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TclError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTclError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-bccce7e00859>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'the little yellow bird is flying in the sky'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mgrammar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"NP: {<DT>?<JJ>*<NN>}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mchunking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrammar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-19-bccce7e00859>\u001b[0m in \u001b[0;36mchunking\u001b[0;34m(text, grammar)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msubtree\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubtrees\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubtree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'the little yellow bird is flying in the sky'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tree/tree.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    760\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdraw_trees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 762\u001b[0;31m         \u001b[0mdraw_trees\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpretty_print\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhighlight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/draw/tree.py\u001b[0m in \u001b[0;36mdraw_trees\u001b[0;34m(*trees)\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1007\u001b[0m     \"\"\"\n\u001b[0;32m-> 1008\u001b[0;31m     \u001b[0mTreeView\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtrees\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmainloop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1009\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1010\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/draw/tree.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *trees)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trees\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_top\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_top\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"NLTK\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_top\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"<Control-x>\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdestroy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/tkinter/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, screenName, baseName, className, useTk, sync, use)\u001b[0m\n\u001b[1;32m   2021\u001b[0m                 \u001b[0mbaseName\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbaseName\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2022\u001b[0m         \u001b[0minteractive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2023\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tkinter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscreenName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbaseName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minteractive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwantobjects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0museTk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msync\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2024\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0museTk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2025\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loadtk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTclError\u001b[0m: no display name and no $DISPLAY environment variable"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the given example, grammar, which is defined using a simple regular expression rule. This rule says that an NP (Noun Phrase) chunk should be formed whenever the chunker finds an optional determiner (DT) followed by any number of adjectives (JJ) and then a noun (NN)."
      ],
      "metadata": {
        "id": "DfkmHbftcvuW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Named Entity Recognition:**\n",
        "\n",
        "Named Entity Recognition is used to extract information from unstructured text. It is used to classify entities present in a text into categories like a person, organization, event, places, etc. It gives us detailed knowledge about the text and the relationships between the different entities."
      ],
      "metadata": {
        "id": "3wJWRazheRV3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pdAPsBPYel7F",
        "outputId": "4eb8082e-8763-4238-94ee-cc051be2fe7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag, ne_chunk\n",
        "\n",
        "def named_entity_recognition(text):\n",
        "    # tokenize the text\n",
        "    word_tokens = word_tokenize(text)\n",
        "  \n",
        "    # part of speech tagging of words\n",
        "    word_pos = pos_tag(word_tokens)\n",
        "  \n",
        "    # tree of word entities\n",
        "    print(ne_chunk(word_pos))\n",
        "  \n",
        "text = 'Bill works for GeeksforGeeks so he went to Delhi for a meetup.'\n",
        "named_entity_recognition(text)"
      ],
      "metadata": {
        "id": "eNeCArNpJEdK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65e1ad3a-146e-4799-c3ce-1d3e6225ffe5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  (PERSON Bill/NNP)\n",
            "  works/VBZ\n",
            "  for/IN\n",
            "  (ORGANIZATION GeeksforGeeks/NNP)\n",
            "  so/RB\n",
            "  he/PRP\n",
            "  went/VBD\n",
            "  to/TO\n",
            "  (GPE Delhi/NNP)\n",
            "  for/IN\n",
            "  a/DT\n",
            "  meetup/NN\n",
            "  ./.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sentiment Analysis** is a use case of Natural Language Processing (NLP) and comes under the category of text classification. To put it simply, Sentiment Analysis involves classifying a text into various sentiments, such as positive or negative, Happy, Sad or Neutral, etc. Thus, the ultimate goal of sentiment analysis is to decipher the underlying mood, emotion, or sentiment of a text. This is also known as **Opinion Mining.**\n",
        "\n",
        "From a strict machine learning point of view, this task is nothing but a supervised learning task."
      ],
      "metadata": {
        "id": "NSbaxxEhF80l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "h_SJrukMDL_M"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GUmt2dhoJFAk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HGJP7arWJFEi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RmHttMLlJFGu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UCn5-sYzJFJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Vg9c1wviJFMu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Vbt90XbIJFPE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}